{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f615d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class Settings:\n",
    "    \"\"\"Configuration settings for the AgenticRAG system\"\"\"\n",
    "    \n",
    "    # API Keys\n",
    "    GROQ_API_KEY: str = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "    GOOGLE_API_KEY: str = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "    GOOGLE_CSE_ID: str = os.getenv(\"GOOGLE_CSE_ID\", \"\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    GROQ_MODEL: str = \"llama3-8b-8192\"\n",
    "    GROQ_TEMPERATURE: float = 0.1\n",
    "\n",
    "    OPENAI_MODEL: str = \"gpt-4.1-nano-2025-04-14\"\n",
    "    OPENAI_TEMPERATURE: float = 0.3\n",
    "    OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    \n",
    "    # Embedding Model\n",
    "    EMBEDDING_MODEL: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    OPENAI_EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "    # Vector Store\n",
    "    VECTORSTORE_PATH: str = \"data/vectorstore\"\n",
    "    \n",
    "    # Search Configuration\n",
    "    SEARCH_RESULTS_COUNT: int = 5\n",
    "\n",
    "    SERPER_API_KEY: str = os.getenv(\"SERPER_API_KEY\", \"\")\n",
    "    TAVILY_API_KEY: str = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
    "    \n",
    "    # Query Enhancement\n",
    "    MAX_QUERY_LENGTH: int = 200\n",
    "    \n",
    "    # Routing Configuration\n",
    "    DEFAULT_ROUTE: str = \"DIRECT\"\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls) -> bool:\n",
    "        \"\"\"Validate required settings\"\"\"\n",
    "        required_keys = [\"GROQ_API_KEY\"]\n",
    "        for key in required_keys:\n",
    "            if not getattr(cls, key):\n",
    "                raise ValueError(f\"Missing required setting: {key}\")\n",
    "        return True\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bae99ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed /home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf: 31 chunks created\n",
      "Total chunks: 31\n",
      "\n",
      "First chunk preview:\n",
      "2024 27th International Conference on Computer and Information Technology (ICCIT)\n",
      "20-22 December 2024, Coxâ€™s Bazar, Bangladesh\n",
      "SuicideGuard: An NLP-Based Chrome Extension\n",
      "for Detecting Suicidal Though...\n",
      "File not found: document1.pdf\n",
      "File not found: document2.docx\n",
      "File not found: document3.txt\n",
      "File not found: document4.md\n",
      "\n",
      "Total chunks from all files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.schema import Document\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    A class to read various document types and chunk them using LangChain\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentChunker\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): Size of each chunk in characters\n",
    "            chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def read_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read PDF file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def read_docx(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read DOCX file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def read_txt(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read TXT file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading TXT file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def read_md(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read Markdown file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading MD file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def load_document(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load document based on file extension\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of loaded documents\n",
    "        \"\"\"\n",
    "        file_extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            return self.read_pdf(file_path)\n",
    "        elif file_extension == '.docx':\n",
    "            return self.read_docx(file_path)\n",
    "        elif file_extension == '.txt':\n",
    "            return self.read_txt(file_path)\n",
    "        elif file_extension == '.md':\n",
    "            return self.read_md(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_extension}\")\n",
    "            return []\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk documents and return list of strings\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of chunked text strings\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Extract text content from chunks\n",
    "        chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "        \n",
    "        return chunk_texts\n",
    "    \n",
    "    def process_file(self, file_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a single file: load and chunk it\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the file to process\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of chunked text strings\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Load document\n",
    "        documents = self.load_document(file_path)\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"No content loaded from {file_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunks = self.chunk_documents(documents)\n",
    "        \n",
    "        print(f\"Successfully processed {file_path}: {len(chunks)} chunks created\")\n",
    "        return chunks\n",
    "    \n",
    "    def process_multiple_files(self, file_paths: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process multiple files and return combined chunks\n",
    "        \n",
    "        Args:\n",
    "            file_paths (List[str]): List of file paths to process\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Combined list of chunked text strings\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            chunks = self.process_file(file_path)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "# Example usage and utility functions\n",
    "def main():\n",
    "    \"\"\"Example usage of the DocumentChunker class\"\"\"\n",
    "    \n",
    "    # Initialize chunker with custom parameters\n",
    "    chunker = DocumentChunker(chunk_size=1000, chunk_overlap=150)\n",
    "    \n",
    "    # Example: Process a single file\n",
    "    file_path = \"/home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf\"  # Replace with your file path\n",
    "    chunks = chunker.process_file(file_path)\n",
    "    \n",
    "    if chunks:\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        print(\"\\nFirst chunk preview:\")\n",
    "        print(chunks[0][:200] + \"...\" if len(chunks[0]) > 200 else chunks[0])\n",
    "    \n",
    "    # Example: Process multiple files\n",
    "    file_paths = [\n",
    "        \"document1.pdf\",\n",
    "        \"document2.docx\",\n",
    "        \"document3.txt\",\n",
    "        \"document4.md\"\n",
    "    ]\n",
    "    \n",
    "    all_chunks = chunker.process_multiple_files(file_paths)\n",
    "    print(f\"\\nTotal chunks from all files: {len(all_chunks)}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def create_chunker_with_custom_settings(chunk_size: int = 1000, \n",
    "                                       chunk_overlap: int = 200) -> DocumentChunker:\n",
    "    \"\"\"\n",
    "    Create a DocumentChunker with custom settings\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        DocumentChunker: Configured chunker instance\n",
    "    \"\"\"\n",
    "    return DocumentChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ba21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Union, Literal\n",
    "\n",
    "class EmbeddingFactory:\n",
    "    \"\"\"Factory for creating embedding instances\"\"\"\n",
    "    \n",
    "    _huggingface_instance = None\n",
    "    _openai_instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings(cls, provider: Literal[\"huggingface\", \"openai\"] = \"huggingface\") -> Union[HuggingFaceEmbeddings, OpenAIEmbeddings]:\n",
    "        \"\"\"Get or create embeddings instance (singleton pattern)\"\"\"\n",
    "        if provider == \"huggingface\":\n",
    "            if cls._huggingface_instance is None:\n",
    "                cls._huggingface_instance = HuggingFaceEmbeddings(\n",
    "                    model_name=settings.EMBEDDING_MODEL\n",
    "                )\n",
    "            return cls._huggingface_instance\n",
    "        elif provider == \"openai\":\n",
    "            if cls._openai_instance is None:\n",
    "                cls._openai_instance = OpenAIEmbeddings(\n",
    "                    model=settings.OPENAI_EMBEDDING_MODEL,\n",
    "                    openai_api_key=settings.OPENAI_API_KEY\n",
    "                )\n",
    "            return cls._openai_instance\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_new_embeddings(cls, provider: Literal[\"huggingface\", \"openai\"] = \"huggingface\", **kwargs) -> Union[HuggingFaceEmbeddings, OpenAIEmbeddings]:\n",
    "        \"\"\"Create a new embeddings instance with custom parameters\"\"\"\n",
    "        if provider == \"huggingface\":\n",
    "            return HuggingFaceEmbeddings(\n",
    "                model_name=kwargs.get(\"model_name\", settings.EMBEDDING_MODEL),\n",
    "                **{k: v for k, v in kwargs.items() if k != \"model_name\"}\n",
    "            )\n",
    "        elif provider == \"openai\":\n",
    "            return OpenAIEmbeddings(\n",
    "                model=kwargs.get(\"model\", settings.OPENAI_EMBEDDING_MODEL),\n",
    "                openai_api_key=kwargs.get(\"api_key\", settings.OPENAI_API_KEY),\n",
    "                **{k: v for k, v in kwargs.items() if k not in [\"model\", \"api_key\"]}\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_huggingface_embeddings(cls) -> HuggingFaceEmbeddings:\n",
    "        \"\"\"Convenience method to get HuggingFace embeddings\"\"\"\n",
    "        return cls.get_embeddings(\"huggingface\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_openai_embeddings(cls) -> OpenAIEmbeddings:\n",
    "        \"\"\"Convenience method to get OpenAI embeddings\"\"\"\n",
    "        return cls.get_embeddings(\"openai\")\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_instances(cls):\n",
    "        \"\"\"Reset singleton instances (useful for testing)\"\"\"\n",
    "        cls._huggingface_instance = None\n",
    "        cls._openai_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f04c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from typing import Union, Literal\n",
    "\n",
    "class LLMFactory:\n",
    "    \"\"\"Factory for creating LLM instances\"\"\"\n",
    "    \n",
    "    _groq_instance = None\n",
    "    _openai_instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_llm(cls, provider: Literal[\"groq\", \"openai\"] = \"groq\") -> Union[ChatGroq, ChatOpenAI]:\n",
    "        \"\"\"Get or create LLM instance (singleton pattern)\"\"\"\n",
    "        if provider == \"groq\":\n",
    "            if cls._groq_instance is None:\n",
    "                cls._groq_instance = ChatGroq(\n",
    "                    model=settings.GROQ_MODEL,\n",
    "                    temperature=settings.GROQ_TEMPERATURE,\n",
    "                    groq_api_key=settings.GROQ_API_KEY\n",
    "                )\n",
    "            return cls._groq_instance\n",
    "        elif provider == \"openai\":\n",
    "            if cls._openai_instance is None:\n",
    "                cls._openai_instance = ChatOpenAI(\n",
    "                    model=settings.OPENAI_MODEL,\n",
    "                    temperature=settings.OPENAI_TEMPERATURE,\n",
    "                    openai_api_key=settings.OPENAI_API_KEY\n",
    "                )\n",
    "            return cls._openai_instance\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_new_llm(cls, provider: Literal[\"groq\", \"openai\"] = \"groq\", **kwargs) -> Union[ChatGroq, ChatOpenAI]:\n",
    "        \"\"\"Create a new LLM instance with custom parameters\"\"\"\n",
    "        if provider == \"groq\":\n",
    "            return ChatGroq(\n",
    "                model=kwargs.get(\"model\", settings.GROQ_MODEL),\n",
    "                temperature=kwargs.get(\"temperature\", settings.GROQ_TEMPERATURE),\n",
    "                groq_api_key=kwargs.get(\"api_key\", settings.GROQ_API_KEY)\n",
    "            )\n",
    "        elif provider == \"openai\":\n",
    "            return ChatOpenAI(\n",
    "                model=kwargs.get(\"model\", settings.OPENAI_MODEL),\n",
    "                temperature=kwargs.get(\"temperature\", settings.OPENAI_TEMPERATURE),\n",
    "                openai_api_key=kwargs.get(\"api_key\", settings.OPENAI_API_KEY)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_groq_llm(cls) -> ChatGroq:\n",
    "        \"\"\"Convenience method to get Groq LLM\"\"\"\n",
    "        return cls.get_llm(\"groq\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_openai_llm(cls) -> ChatOpenAI:\n",
    "        \"\"\"Convenience method to get OpenAI LLM\"\"\"\n",
    "        return cls.get_llm(\"openai\")\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_instances(cls):\n",
    "        \"\"\"Reset singleton instances (useful for testing)\"\"\"\n",
    "        cls._groq_instance = None\n",
    "        cls._openai_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee12c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.tools import GoogleSerperRun\n",
    "from typing import Union, Literal\n",
    "\n",
    "class SearchToolFactory:\n",
    "    \"\"\"Factory for creating search tools\"\"\"\n",
    "    \n",
    "    _tavily_instance = None\n",
    "    _serper_instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_search_tool(cls, provider: Literal[\"tavily\", \"serper\"] = \"tavily\") -> Union[TavilySearchResults, GoogleSerperRun]:\n",
    "        \"\"\"Get or create search tool instance (singleton pattern)\"\"\"\n",
    "        if provider == \"tavily\":\n",
    "            if cls._tavily_instance is None:\n",
    "                cls._tavily_instance = TavilySearchResults(\n",
    "                    api_key=settings.TAVILY_API_KEY\n",
    "                )\n",
    "            return cls._tavily_instance\n",
    "        elif provider == \"serper\":\n",
    "            if cls._serper_instance is None:\n",
    "                search_wrapper = GoogleSerperAPIWrapper(\n",
    "                    serper_api_key=settings.SERPER_API_KEY\n",
    "                )\n",
    "                cls._serper_instance = GoogleSerperRun(api_wrapper=search_wrapper)\n",
    "            return cls._serper_instance\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_new_search_tool(cls, provider: Literal[\"tavily\", \"serper\"] = \"tavily\", **kwargs) -> Union[TavilySearchResults, GoogleSerperRun]:\n",
    "        \"\"\"Create a new search tool instance with custom parameters\"\"\"\n",
    "        if provider == \"tavily\":\n",
    "            return TavilySearchResults(\n",
    "                api_key=kwargs.get(\"api_key\", settings.TAVILY_API_KEY),\n",
    "                max_results=kwargs.get(\"max_results\", settings.SEARCH_RESULTS_COUNT),\n",
    "                search_depth=kwargs.get(\"search_depth\", settings.TAVILY_SEARCH_DEPTH),\n",
    "                include_answer=kwargs.get(\"include_answer\", settings.TAVILY_INCLUDE_ANSWER),\n",
    "                include_raw_content=kwargs.get(\"include_raw_content\", settings.TAVILY_INCLUDE_RAW_CONTENT),\n",
    "                **{k: v for k, v in kwargs.items() if k not in [\"api_key\", \"max_results\", \"search_depth\", \"include_answer\", \"include_raw_content\"]}\n",
    "            )\n",
    "        elif provider == \"serper\":\n",
    "            search_wrapper = GoogleSerperAPIWrapper(\n",
    "                serper_api_key=kwargs.get(\"api_key\", settings.SERPER_API_KEY),\n",
    "                k=kwargs.get(\"k\", settings.SEARCH_RESULTS_COUNT),\n",
    "                type=kwargs.get(\"type\", settings.SERPER_SEARCH_TYPE),\n",
    "                country=kwargs.get(\"country\", settings.SERPER_COUNTRY),\n",
    "                location=kwargs.get(\"location\", settings.SERPER_LOCATION),\n",
    "                **{k: v for k, v in kwargs.items() if k not in [\"api_key\", \"k\", \"type\", \"country\", \"location\"]}\n",
    "            )\n",
    "            return GoogleSerperRun(api_wrapper=search_wrapper)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_tavily_search(cls) -> TavilySearchResults:\n",
    "        \"\"\"Convenience method to get Tavily search tool\"\"\"\n",
    "        return cls.get_search_tool(\"tavily\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_serper_search(cls) -> GoogleSerperRun:\n",
    "        \"\"\"Convenience method to get Serper search tool\"\"\"\n",
    "        return cls.get_search_tool(\"serper\")\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_instances(cls):\n",
    "        \"\"\"Reset singleton instances (useful for testing)\"\"\"\n",
    "        cls._tavily_instance = None\n",
    "        cls._serper_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a11dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface  import HuggingFaceEmbeddings\n",
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manager for vector store operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = EmbeddingFactory.get_embeddings()\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def load_vectorstore(self, path: Optional[str] = None) -> bool:\n",
    "        \"\"\"Load vector store from path\"\"\"\n",
    "        try:\n",
    "            path = path or settings.VECTORSTORE_PATH\n",
    "            if os.path.exists(path):\n",
    "                self.vectorstore = FAISS.load_local(path, self.embeddings)\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vectorstore: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search_documents(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            return [doc.page_content for doc in docs]\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def add_documents(self, texts: List[str], metadatas: Optional[List[dict]] = None):\n",
    "        \"\"\"Add documents to vector store\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            self.vectorstore = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
    "        else:\n",
    "            self.vectorstore.add_texts(texts, metadatas=metadatas)\n",
    "    \n",
    "    def save_vectorstore(self, path: Optional[str] = None):\n",
    "        \"\"\"Save vector store to path\"\"\"\n",
    "        if self.vectorstore:\n",
    "            path = path or settings.VECTORSTORE_PATH\n",
    "            self.vectorstore.save_local(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b5929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    \"\"\"Request schema for query processing\"\"\"\n",
    "    query: str\n",
    "    session_id: Optional[str] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    \"\"\"Response schema for query processing\"\"\"\n",
    "    query: str\n",
    "    upgraded_query: str\n",
    "    route_taken: str\n",
    "    response: str\n",
    "    metadata: Dict[str, Any]\n",
    "    processing_time: float\n",
    "\n",
    "class ProcessingMetadata(BaseModel):\n",
    "    \"\"\"Metadata for processing steps\"\"\"\n",
    "    upgrade_success: bool = False\n",
    "    routing_success: bool = False\n",
    "    path_success: bool = False\n",
    "    errors: List[str] = []\n",
    "    processing_time: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56a10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"State schema for the AgenticRAG workflow\"\"\"\n",
    "    \n",
    "    user_query: str = Field(description=\"Original user query\")\n",
    "    upgraded_query: str = Field(default=\"\", description=\"Enhanced query\")\n",
    "    route_decision: str = Field(default=\"\", description=\"Routing decision\")\n",
    "    retrieved_docs: List[str] = Field(default_factory=list, description=\"Retrieved documents\")\n",
    "    search_results: List[str] = Field(default_factory=list, description=\"Web search results\")\n",
    "    final_response: str = Field(default=\"\", description=\"Final response\")\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Pydantic configuration\"\"\"\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4e83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class Prompts:\n",
    "    \"\"\"Centralized prompt templates\"\"\"\n",
    "    \n",
    "    QUERY_UPGRADER = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a query enhancement specialist. Your task is to improve user queries for better information retrieval.\n",
    "        \n",
    "        Enhancement guidelines:\n",
    "        1. Add relevant keywords and synonyms\n",
    "        2. Clarify ambiguous terms\n",
    "        3. Expand abbreviations and acronyms\n",
    "        4. Add context when missing\n",
    "        5. Maintain original intent\n",
    "        6. Keep enhanced query concise (under 200 characters)\n",
    "        \n",
    "        Return only the enhanced query, nothing else.\"\"\"),\n",
    "        (\"human\", \"Original query: {query}\")\n",
    "    ])\n",
    "    \n",
    "    QUERY_ROUTER = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a query router. Analyze the query and decide which path to take:\n",
    "\n",
    "        PATHS:\n",
    "        1. \"RAG\" - For queries about specific knowledge base content, documents, or domain expertise\n",
    "        2. \"WEB\" - For current events, real-time information, recent news, or trending topics\n",
    "        3. \"DIRECT\" - For general conversation, creative tasks, opinions, or reasoning without specific facts\n",
    "\n",
    "        DECISION CRITERIA:\n",
    "        - RAG: Domain-specific questions, technical documentation, specific facts from knowledge base\n",
    "        - WEB: Questions with temporal keywords (latest, current, recent, today), current events, real-time data\n",
    "        - DIRECT: General chat, creative writing, opinions, mathematical reasoning, casual conversation\n",
    "\n",
    "        Respond with only one word: RAG, WEB, or DIRECT\"\"\"),\n",
    "        (\"human\", \"Query: {query}\")\n",
    "    ])\n",
    "    \n",
    "    RAG_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the user's question based on the provided context from the knowledge base.\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        If the context doesn't contain relevant information, say so clearly.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    WEB_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the user's question based on the provided web search results.\n",
    "        \n",
    "        Search Results: {search_results}\n",
    "        \n",
    "        Provide a comprehensive answer based on the search results. If the results don't contain relevant information, say so clearly.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    DIRECT_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant. Answer the user's question directly using your knowledge and reasoning capabilities.\n",
    "        \n",
    "        Be conversational, accurate, and helpful. If you're unsure about something, acknowledge the uncertainty.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58e6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectLLMNode:\n",
    "    \"\"\"Node for direct LLM processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.prompt = Prompts.DIRECT_RESPONSE\n",
    "    \n",
    "    def process_direct_llm(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process direct LLM path\"\"\"\n",
    "        \n",
    "        try:\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            response = chain.invoke({\"query\": state.upgraded_query})\n",
    "            state.final_response = response.content\n",
    "            state.metadata[\"direct_llm_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.final_response = \"Sorry, I couldn't process your request at the moment.\"\n",
    "            state.metadata[\"direct_llm_success\"] = False\n",
    "            state.metadata[\"direct_llm_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def direct_llm_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for direct LLM processing\"\"\"\n",
    "    direct_processor = DirectLLMNode()\n",
    "    return direct_processor.process_direct_llm(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0c087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRouter:\n",
    "    \"\"\"Node for routing queries to appropriate paths\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.prompt = Prompts.QUERY_ROUTER\n",
    "    \n",
    "    def route_query(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Route query to appropriate path\"\"\"\n",
    "        \n",
    "        chain = self.prompt | self.llm\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": state.upgraded_query})\n",
    "            route_decision = response.content.strip().upper()\n",
    "            \n",
    "            # Validate route decision\n",
    "            if route_decision not in [\"RAG\", \"WEB\", \"DIRECT\"]:\n",
    "                route_decision = settings.DEFAULT_ROUTE\n",
    "                \n",
    "            state.route_decision = route_decision\n",
    "            state.metadata[\"routing_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.route_decision = settings.DEFAULT_ROUTE\n",
    "            state.metadata[\"routing_success\"] = False\n",
    "            state.metadata[\"routing_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def query_router_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for query routing\"\"\"\n",
    "    router = QueryRouter()\n",
    "    return router.route_query(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c517d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryUpgrader:\n",
    "    \"\"\"Node for upgrading user queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.prompt = Prompts.QUERY_UPGRADER\n",
    "    \n",
    "    def upgrade_query(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Upgrade/enhance the user query\"\"\"\n",
    "        \n",
    "        chain = self.prompt | self.llm\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": state.user_query})\n",
    "            upgraded_query = response.content.strip()\n",
    "            \n",
    "            # Fallback to original if upgrade fails\n",
    "            if not upgraded_query or len(upgraded_query) > settings.MAX_QUERY_LENGTH:\n",
    "                upgraded_query = state.user_query\n",
    "                \n",
    "            state.upgraded_query = upgraded_query\n",
    "            state.metadata[\"upgrade_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.upgraded_query = state.user_query\n",
    "            state.metadata[\"upgrade_success\"] = False\n",
    "            state.metadata[\"upgrade_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def query_upgrader_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for query upgrading\"\"\"\n",
    "    upgrader = QueryUpgrader()\n",
    "    return upgrader.upgrade_query(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a93ee406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGNode:\n",
    "    \"\"\"Node for RAG processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.vectorstore_manager = VectorStoreManager()\n",
    "        self.prompt = Prompts.RAG_RESPONSE\n",
    "        \n",
    "        # Load vectorstore\n",
    "        self.vectorstore_manager.load_vectorstore()\n",
    "    \n",
    "    def process_rag(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process RAG path - retrieve from knowledge base\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            docs = self.vectorstore_manager.search_documents(state.upgraded_query, k=3)\n",
    "            state.retrieved_docs = docs\n",
    "            \n",
    "            # Generate response with retrieved context\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            context = \"\\n\".join(docs) if docs else \"No relevant documents found.\"\n",
    "            response = chain.invoke({\n",
    "                \"query\": state.upgraded_query,\n",
    "                \"context\": context\n",
    "            })\n",
    "            \n",
    "            state.final_response = response.content\n",
    "            state.metadata[\"rag_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.final_response = \"Sorry, I couldn't retrieve information from the knowledge base.\"\n",
    "            state.metadata[\"rag_success\"] = False\n",
    "            state.metadata[\"rag_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def rag_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for RAG processing\"\"\"\n",
    "    rag_processor = RAGNode()\n",
    "    return rag_processor.process_rag(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13311fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchNode:\n",
    "    \"\"\"Node for web search processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.search_tool = SearchToolFactory.get_search_tool()\n",
    "        self.prompt = Prompts.WEB_RESPONSE\n",
    "    \n",
    "    def process_web_search(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process web search path\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Perform web search\n",
    "            search_results = self.search_tool.run(state.upgraded_query)\n",
    "            state.search_results = [search_results]\n",
    "            \n",
    "            # Generate response with search results\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            response = chain.invoke({\n",
    "                \"query\": state.upgraded_query,\n",
    "                \"search_results\": search_results\n",
    "            })\n",
    "            \n",
    "            state.final_response = response.content\n",
    "            state.metadata[\"web_search_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.final_response = \"Sorry, I couldn't perform web search at the moment.\"\n",
    "            state.metadata[\"web_search_success\"] = False\n",
    "            state.metadata[\"web_search_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def web_search_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for web search processing\"\"\"\n",
    "    web_processor = WebSearchNode()\n",
    "    return web_processor.process_web_search(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bbcdd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(state: AgentState) -> Literal[\"rag_path\", \"web_search\", \"direct_llm\"]:\n",
    "    \"\"\"Route to appropriate path based on decision\"\"\"\n",
    "    route_map = {\n",
    "        \"RAG\": \"rag_path\",\n",
    "        \"WEB\": \"web_search\", \n",
    "        \"DIRECT\": \"direct_llm\"\n",
    "    }\n",
    "    return route_map.get(state.route_decision, \"direct_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe42640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class GraphBuilder:\n",
    "    \"\"\"Builder for the AgenticRAG graph\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_graph():\n",
    "        \"\"\"Create the LangGraph workflow\"\"\"\n",
    "        \n",
    "        # Initialize graph\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"query_upgrader\", query_upgrader_node)\n",
    "        workflow.add_node(\"query_router\", query_router_node)\n",
    "        workflow.add_node(\"rag_path\", rag_node)\n",
    "        workflow.add_node(\"web_search\", web_search_node)\n",
    "        workflow.add_node(\"direct_llm\", direct_llm_node)\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"query_upgrader\")\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"query_upgrader\", \"query_router\")\n",
    "        \n",
    "        # Add conditional edges based on routing decision\n",
    "        workflow.add_conditional_edges(\n",
    "            \"query_router\",\n",
    "            route_query,\n",
    "            {\n",
    "                \"rag_path\": \"rag_path\",\n",
    "                \"web_search\": \"web_search\",\n",
    "                \"direct_llm\": \"direct_llm\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # All paths end at END\n",
    "        workflow.add_edge(\"rag_path\", END)\n",
    "        workflow.add_edge(\"web_search\", END)\n",
    "        workflow.add_edge(\"direct_llm\", END)\n",
    "        \n",
    "        # Compile the graph\n",
    "        return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b304f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-12 19:38:38.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAgenticRAG system initialized successfully\u001b[0m\n",
      "\u001b[32m2025-07-12 19:38:38.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing query: What is machine learning?\u001b[0m\n",
      "\u001b[32m2025-07-12 19:38:38.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mInitial state created: user_query='What is machine learning?' upgraded_query='' route_decision='' retrieved_docs=[] search_results=[] final_response='' metadata={}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: What is machine learning?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-12 19:38:39.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mFinal state after processing: {'user_query': 'What is machine learning?', 'upgraded_query': 'What is artificial intelligence machine learning algorithm?', 'route_decision': 'RAG', 'retrieved_docs': [], 'search_results': [], 'final_response': \"I apologize, but since there are no relevant documents found in my knowledge base, I don't have any information on what artificial intelligence machine learning algorithm is. If you could provide more context or clarify your question, I'll do my best to help you.\", 'metadata': {'upgrade_success': True, 'routing_success': True, 'rag_success': True}}\u001b[0m\n",
      "\u001b[32m2025-07-12 19:38:39.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mQuery processed successfully in 0.96s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What is machine learning?\n",
      "Upgraded Query: What is artificial intelligence machine learning algorithm?\n",
      "Route Taken: RAG\n",
      "Response: I apologize, but since there are no relevant documents found in my knowledge base, I don't have any information on what artificial intelligence machine learning algorithm is. If you could provide more context or clarify your question, I'll do my best to help you.\n",
      "Processing Time: 0.96s\n",
      "Metadata: {'upgrade_success': True, 'routing_success': True, 'rag_success': True}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from loguru import logger\n",
    "\n",
    "class AgenticRAGSystem:\n",
    "    \"\"\"Main AgenticRAG system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Validate settings\n",
    "        settings.validate()\n",
    "        \n",
    "        # Create graph\n",
    "        self.app = GraphBuilder.create_graph()\n",
    "        \n",
    "        logger.info(\"AgenticRAG system initialized successfully\")\n",
    "    \n",
    "    def process_query(self, query: str) -> QueryResponse:\n",
    "        \"\"\"Process a single query\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "\n",
    "        try:\n",
    "            # Initialize state\n",
    "            initial_state = AgentState(user_query=query)\n",
    "            logger.info(f\"Initial state created: {initial_state}\")\n",
    "            \n",
    "            # Run the graph\n",
    "            final_state = self.app.invoke(initial_state)\n",
    "            logger.info(f\"Final state after processing: {final_state}\")\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Create response\n",
    "            response = QueryResponse(\n",
    "                query=final_state['user_query'],\n",
    "                upgraded_query=final_state['upgraded_query'],\n",
    "                route_taken=final_state['route_decision'],\n",
    "                response=final_state['final_response'],\n",
    "                metadata=final_state['metadata'],\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Query processed successfully in {processing_time:.2f}s\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def process_batch(self, queries: List[str]) -> List[QueryResponse]:\n",
    "        \"\"\"Process multiple queries\"\"\"\n",
    "        \n",
    "        responses = []\n",
    "        for query in queries:\n",
    "            try:\n",
    "                response = self.process_query(query)\n",
    "                responses.append(response)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing query '{query}': {e}\")\n",
    "        \n",
    "        return responses\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    \n",
    "    # Initialize system\n",
    "    system = AgenticRAGSystem()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"What is machine learning?\",\n",
    "        # \"Latest news about AI\",\n",
    "        # \"Write a poem about spring\"\n",
    "    ]\n",
    "    \n",
    "    # Process queries\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            response = system.process_query(query)\n",
    "            \n",
    "            print(f\"Original Query: {response.query}\")\n",
    "            print(f\"Upgraded Query: {response.upgraded_query}\")\n",
    "            print(f\"Route Taken: {response.route_taken}\")\n",
    "            print(f\"Response: {response.response}\")\n",
    "            print(f\"Processing Time: {response.processing_time:.2f}s\")\n",
    "            print(f\"Metadata: {response.metadata}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee3505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
