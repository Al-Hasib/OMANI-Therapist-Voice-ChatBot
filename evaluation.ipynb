{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eca1a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing required setting: GROQ_API_KEY",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magenticRAG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m agenticRAGResponse\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43magenticRAGResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\abdullah\\Desktop\\medium\\OMANI-Therapist-Voice-ChatBot\\src\\agenticRAG\\main.py:69\u001b[0m, in \u001b[0;36magenticRAGResponse\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenticRAGResponse\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QueryResponse:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Function to get response for a single query\"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     system \u001b[38;5;241m=\u001b[39m \u001b[43mAgenticRAGSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m system\u001b[38;5;241m.\u001b[39mprocess_query(query)\n",
      "File \u001b[1;32mc:\\Users\\abdullah\\Desktop\\medium\\OMANI-Therapist-Voice-ChatBot\\src\\agenticRAG\\main.py:14\u001b[0m, in \u001b[0;36mAgenticRAGSystem.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Validate settings\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Create graph\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp \u001b[38;5;241m=\u001b[39m GraphBuilder\u001b[38;5;241m.\u001b[39mcreate_graph()\n",
      "File \u001b[1;32mc:\\Users\\abdullah\\Desktop\\medium\\OMANI-Therapist-Voice-ChatBot\\src\\config\\settings.py:46\u001b[0m, in \u001b[0;36mSettings.validate\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m required_keys:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, key):\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required setting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Missing required setting: GROQ_API_KEY"
     ]
    }
   ],
   "source": [
    "from src.agenticRAG.main import agenticRAGResponse\n",
    "\n",
    "data = agenticRAGResponse(query=\"What is the capital of France?\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f615d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class Settings:\n",
    "    \"\"\"Configuration settings for the AgenticRAG system\"\"\"\n",
    "    \n",
    "    # API Keys\n",
    "    GROQ_API_KEY: str = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "    GOOGLE_API_KEY: str = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "    GOOGLE_CSE_ID: str = os.getenv(\"GOOGLE_CSE_ID\", \"\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    GROQ_MODEL: str = \"llama3-8b-8192\"\n",
    "    GROQ_TEMPERATURE: float = 0.1\n",
    "\n",
    "    OPENAI_MODEL: str = \"gpt-4.1-nano-2025-04-14\"\n",
    "    OPENAI_TEMPERATURE: float = 0.3\n",
    "    OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    \n",
    "    # Embedding Model\n",
    "    EMBEDDING_MODEL: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    OPENAI_EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "    # Vector Store\n",
    "    VECTORSTORE_PATH: str = \"data/vectorstore\"\n",
    "    \n",
    "    # Search Configuration\n",
    "    SEARCH_RESULTS_COUNT: int = 5\n",
    "\n",
    "    SERPER_API_KEY: str = os.getenv(\"SERPER_API_KEY\", \"\")\n",
    "    TAVILY_API_KEY: str = os.getenv(\"TAVILY_API_KEY\", \"\")\n",
    "    \n",
    "    # Query Enhancement\n",
    "    MAX_QUERY_LENGTH: int = 200\n",
    "    \n",
    "    # Routing Configuration\n",
    "    DEFAULT_ROUTE: str = \"DIRECT\"\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls) -> bool:\n",
    "        \"\"\"Validate required settings\"\"\"\n",
    "        required_keys = [\"GROQ_API_KEY\"]\n",
    "        for key in required_keys:\n",
    "            if not getattr(cls, key):\n",
    "                raise ValueError(f\"Missing required setting: {key}\")\n",
    "        return True\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bae99ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed /home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf: 31 chunks created\n",
      "Total chunks: 31\n",
      "\n",
      "First chunk preview:\n",
      "2024 27th International Conference on Computer and Information Technology (ICCIT)\n",
      "20-22 December 2024, Coxâ€™s Bazar, Bangladesh\n",
      "SuicideGuard: An NLP-Based Chrome Extension\n",
      "for Detecting Suicidal Though...\n",
      "File not found: document1.pdf\n",
      "File not found: document2.docx\n",
      "File not found: document3.txt\n",
      "File not found: document4.md\n",
      "\n",
      "Total chunks from all files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.schema import Document\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    A class to read various document types and chunk them using LangChain\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentChunker\n",
    "        \n",
    "        Args:\n",
    "            chunk_size (int): Size of each chunk in characters\n",
    "            chunk_overlap (int): Number of characters to overlap between chunks\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def read_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read PDF file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def read_docx(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read DOCX file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def read_txt(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read TXT file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading TXT file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def read_md(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Read Markdown file and return documents\"\"\"\n",
    "        try:\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading MD file {file_path}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def load_document(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load document based on file extension\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of loaded documents\n",
    "        \"\"\"\n",
    "        file_extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            return self.read_pdf(file_path)\n",
    "        elif file_extension == '.docx':\n",
    "            return self.read_docx(file_path)\n",
    "        elif file_extension == '.txt':\n",
    "            return self.read_txt(file_path)\n",
    "        elif file_extension == '.md':\n",
    "            return self.read_md(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_extension}\")\n",
    "            return []\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk documents and return list of strings\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of chunked text strings\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Extract text content from chunks\n",
    "        chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "        \n",
    "        return chunk_texts\n",
    "    \n",
    "    def process_file(self, file_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a single file: load and chunk it\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the file to process\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of chunked text strings\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Load document\n",
    "        documents = self.load_document(file_path)\n",
    "        \n",
    "        if not documents:\n",
    "            print(f\"No content loaded from {file_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunks = self.chunk_documents(documents)\n",
    "        \n",
    "        print(f\"Successfully processed {file_path}: {len(chunks)} chunks created\")\n",
    "        return chunks\n",
    "    \n",
    "    def process_multiple_files(self, file_paths: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process multiple files and return combined chunks\n",
    "        \n",
    "        Args:\n",
    "            file_paths (List[str]): List of file paths to process\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: Combined list of chunked text strings\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            chunks = self.process_file(file_path)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "# Example usage and utility functions\n",
    "def main():\n",
    "    \"\"\"Example usage of the DocumentChunker class\"\"\"\n",
    "    \n",
    "    # Initialize chunker with custom parameters\n",
    "    chunker = DocumentChunker(chunk_size=1000, chunk_overlap=150)\n",
    "    \n",
    "    # Example: Process a single file\n",
    "    file_path = \"/home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf\"  # Replace with your file path\n",
    "    chunks = chunker.process_file(file_path)\n",
    "    \n",
    "    if chunks:\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        print(\"\\nFirst chunk preview:\")\n",
    "        print(chunks[0][:200] + \"...\" if len(chunks[0]) > 200 else chunks[0])\n",
    "    \n",
    "    # Example: Process multiple files\n",
    "    file_paths = [\n",
    "        \"document1.pdf\",\n",
    "        \"document2.docx\",\n",
    "        \"document3.txt\",\n",
    "        \"document4.md\"\n",
    "    ]\n",
    "    \n",
    "    all_chunks = chunker.process_multiple_files(file_paths)\n",
    "    print(f\"\\nTotal chunks from all files: {len(all_chunks)}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def create_chunker_with_custom_settings(chunk_size: int = 1000, \n",
    "                                       chunk_overlap: int = 200) -> DocumentChunker:\n",
    "    \"\"\"\n",
    "    Create a DocumentChunker with custom settings\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        DocumentChunker: Configured chunker instance\n",
    "    \"\"\"\n",
    "    return DocumentChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ba21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Union, Literal\n",
    "\n",
    "class EmbeddingFactory:\n",
    "    \"\"\"Factory for creating embedding instances\"\"\"\n",
    "    \n",
    "    _huggingface_instance = None\n",
    "    _openai_instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings(cls, provider: Literal[\"huggingface\", \"openai\"] = \"huggingface\") -> Union[HuggingFaceEmbeddings, OpenAIEmbeddings]:\n",
    "        \"\"\"Get or create embeddings instance (singleton pattern)\"\"\"\n",
    "        if provider == \"huggingface\":\n",
    "            if cls._huggingface_instance is None:\n",
    "                cls._huggingface_instance = HuggingFaceEmbeddings(\n",
    "                    model_name=settings.EMBEDDING_MODEL\n",
    "                )\n",
    "            return cls._huggingface_instance\n",
    "        elif provider == \"openai\":\n",
    "            if cls._openai_instance is None:\n",
    "                cls._openai_instance = OpenAIEmbeddings(\n",
    "                    model=settings.OPENAI_EMBEDDING_MODEL,\n",
    "                    openai_api_key=settings.OPENAI_API_KEY\n",
    "                )\n",
    "            return cls._openai_instance\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_new_embeddings(cls, provider: Literal[\"huggingface\", \"openai\"] = \"huggingface\", **kwargs) -> Union[HuggingFaceEmbeddings, OpenAIEmbeddings]:\n",
    "        \"\"\"Create a new embeddings instance with custom parameters\"\"\"\n",
    "        if provider == \"huggingface\":\n",
    "            return HuggingFaceEmbeddings(\n",
    "                model_name=kwargs.get(\"model_name\", settings.EMBEDDING_MODEL),\n",
    "                **{k: v for k, v in kwargs.items() if k != \"model_name\"}\n",
    "            )\n",
    "        elif provider == \"openai\":\n",
    "            return OpenAIEmbeddings(\n",
    "                model=kwargs.get(\"model\", settings.OPENAI_EMBEDDING_MODEL),\n",
    "                openai_api_key=kwargs.get(\"api_key\", settings.OPENAI_API_KEY),\n",
    "                **{k: v for k, v in kwargs.items() if k not in [\"model\", \"api_key\"]}\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_huggingface_embeddings(cls) -> HuggingFaceEmbeddings:\n",
    "        \"\"\"Convenience method to get HuggingFace embeddings\"\"\"\n",
    "        return cls.get_embeddings(\"huggingface\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_openai_embeddings(cls) -> OpenAIEmbeddings:\n",
    "        \"\"\"Convenience method to get OpenAI embeddings\"\"\"\n",
    "        return cls.get_embeddings(\"openai\")\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_instances(cls):\n",
    "        \"\"\"Reset singleton instances (useful for testing)\"\"\"\n",
    "        cls._huggingface_instance = None\n",
    "        cls._openai_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f04c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from typing import Union, Literal\n",
    "\n",
    "class LLMFactory:\n",
    "    \"\"\"Factory for creating LLM instances\"\"\"\n",
    "    \n",
    "    _groq_instance = None\n",
    "    _openai_instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_llm(cls, provider: Literal[\"groq\", \"openai\"] = \"groq\") -> Union[ChatGroq, ChatOpenAI]:\n",
    "        \"\"\"Get or create LLM instance (singleton pattern)\"\"\"\n",
    "        if provider == \"groq\":\n",
    "            if cls._groq_instance is None:\n",
    "                cls._groq_instance = ChatGroq(\n",
    "                    model=settings.GROQ_MODEL,\n",
    "                    temperature=settings.GROQ_TEMPERATURE,\n",
    "                    groq_api_key=settings.GROQ_API_KEY\n",
    "                )\n",
    "            return cls._groq_instance\n",
    "        elif provider == \"openai\":\n",
    "            if cls._openai_instance is None:\n",
    "                cls._openai_instance = ChatOpenAI(\n",
    "                    model=settings.OPENAI_MODEL,\n",
    "                    temperature=settings.OPENAI_TEMPERATURE,\n",
    "                    openai_api_key=settings.OPENAI_API_KEY\n",
    "                )\n",
    "            return cls._openai_instance\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_new_llm(cls, provider: Literal[\"groq\", \"openai\"] = \"groq\", **kwargs) -> Union[ChatGroq, ChatOpenAI]:\n",
    "        \"\"\"Create a new LLM instance with custom parameters\"\"\"\n",
    "        if provider == \"groq\":\n",
    "            return ChatGroq(\n",
    "                model=kwargs.get(\"model\", settings.GROQ_MODEL),\n",
    "                temperature=kwargs.get(\"temperature\", settings.GROQ_TEMPERATURE),\n",
    "                groq_api_key=kwargs.get(\"api_key\", settings.GROQ_API_KEY)\n",
    "            )\n",
    "        elif provider == \"openai\":\n",
    "            return ChatOpenAI(\n",
    "                model=kwargs.get(\"model\", settings.OPENAI_MODEL),\n",
    "                temperature=kwargs.get(\"temperature\", settings.OPENAI_TEMPERATURE),\n",
    "                openai_api_key=kwargs.get(\"api_key\", settings.OPENAI_API_KEY)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_groq_llm(cls) -> ChatGroq:\n",
    "        \"\"\"Convenience method to get Groq LLM\"\"\"\n",
    "        return cls.get_llm(\"groq\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_openai_llm(cls) -> ChatOpenAI:\n",
    "        \"\"\"Convenience method to get OpenAI LLM\"\"\"\n",
    "        return cls.get_llm(\"openai\")\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_instances(cls):\n",
    "        \"\"\"Reset singleton instances (useful for testing)\"\"\"\n",
    "        cls._groq_instance = None\n",
    "        cls._openai_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.tools import GoogleSerperRun\n",
    "from typing import Union, Literal\n",
    "\n",
    "class SearchToolFactory:\n",
    "    \"\"\"Factory for creating search tools\"\"\"\n",
    "    \n",
    "    _tavily_instance = None\n",
    "    _serper_instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_search_tool(cls, provider: Literal[\"tavily\", \"serper\"] = \"tavily\") -> Union[TavilySearch, GoogleSerperRun]:\n",
    "        \"\"\"Get or create search tool instance (singleton pattern)\"\"\"\n",
    "        if provider == \"tavily\":\n",
    "            if cls._tavily_instance is None:\n",
    "                cls._tavily_instance = TavilySearch(\n",
    "                    api_key=settings.TAVILY_API_KEY\n",
    "                )\n",
    "            return cls._tavily_instance\n",
    "        elif provider == \"serper\":\n",
    "            if cls._serper_instance is None:\n",
    "                search_wrapper = GoogleSerperAPIWrapper(\n",
    "                    serper_api_key=settings.SERPER_API_KEY\n",
    "                )\n",
    "                cls._serper_instance = GoogleSerperRun(api_wrapper=search_wrapper)\n",
    "            return cls._serper_instance\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_new_search_tool(cls, provider: Literal[\"tavily\", \"serper\"] = \"tavily\", **kwargs) -> Union[TavilySearchResults, GoogleSerperRun]:\n",
    "        \"\"\"Create a new search tool instance with custom parameters\"\"\"\n",
    "        if provider == \"tavily\":\n",
    "            return TavilySearch(\n",
    "                api_key=kwargs.get(\"api_key\", settings.TAVILY_API_KEY),\n",
    "                max_results=kwargs.get(\"max_results\", settings.SEARCH_RESULTS_COUNT),\n",
    "                search_depth=kwargs.get(\"search_depth\", settings.TAVILY_SEARCH_DEPTH),\n",
    "                include_answer=kwargs.get(\"include_answer\", settings.TAVILY_INCLUDE_ANSWER),\n",
    "                include_raw_content=kwargs.get(\"include_raw_content\", settings.TAVILY_INCLUDE_RAW_CONTENT),\n",
    "                **{k: v for k, v in kwargs.items() if k not in [\"api_key\", \"max_results\", \"search_depth\", \"include_answer\", \"include_raw_content\"]}\n",
    "            )\n",
    "        elif provider == \"serper\":\n",
    "            search_wrapper = GoogleSerperAPIWrapper(\n",
    "                serper_api_key=kwargs.get(\"api_key\", settings.SERPER_API_KEY),\n",
    "                k=kwargs.get(\"k\", settings.SEARCH_RESULTS_COUNT),\n",
    "                type=kwargs.get(\"type\", settings.SERPER_SEARCH_TYPE),\n",
    "                country=kwargs.get(\"country\", settings.SERPER_COUNTRY),\n",
    "                location=kwargs.get(\"location\", settings.SERPER_LOCATION),\n",
    "                **{k: v for k, v in kwargs.items() if k not in [\"api_key\", \"k\", \"type\", \"country\", \"location\"]}\n",
    "            )\n",
    "            return GoogleSerperRun(api_wrapper=search_wrapper)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_tavily_search(cls) -> TavilySearch:\n",
    "        \"\"\"Convenience method to get Tavily search tool\"\"\"\n",
    "        return cls.get_search_tool(\"tavily\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_serper_search(cls) -> GoogleSerperRun:\n",
    "        \"\"\"Convenience method to get Serper search tool\"\"\"\n",
    "        return cls.get_search_tool(\"serper\")\n",
    "    \n",
    "    @classmethod\n",
    "    def reset_instances(cls):\n",
    "        \"\"\"Reset singleton instances (useful for testing)\"\"\"\n",
    "        cls._tavily_instance = None\n",
    "        cls._serper_instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a11dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface  import HuggingFaceEmbeddings\n",
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manager for vector store operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = EmbeddingFactory.get_embeddings()\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def load_vectorstore(self, path: Optional[str] = None) -> bool:\n",
    "        \"\"\"Load vector store from path\"\"\"\n",
    "        try:\n",
    "            path = path or settings.VECTORSTORE_PATH\n",
    "            if os.path.exists(path):\n",
    "                self.vectorstore = FAISS.load_local(path, self.embeddings)\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vectorstore: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search_documents(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            return [doc.page_content for doc in docs]\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def add_documents(self, texts: List[str], metadatas: Optional[List[dict]] = None):\n",
    "        \"\"\"Add documents to vector store\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            self.vectorstore = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
    "        else:\n",
    "            self.vectorstore.add_texts(texts, metadatas=metadatas)\n",
    "    \n",
    "    def save_vectorstore(self, path: Optional[str] = None):\n",
    "        \"\"\"Save vector store to path\"\"\"\n",
    "        if self.vectorstore:\n",
    "            path = path or settings.VECTORSTORE_PATH\n",
    "            self.vectorstore.save_local(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6687924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Storing Single Document ===\n",
      "Processing file: /home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf\n",
      "Successfully processed /home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf: 31 chunks created\n",
      "Successfully processed /home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf: 31 chunks\n",
      "Vector store saved with 31 total chunks\n",
      "Single document processing: Success\n",
      "\n",
      "=== Searching Vector Store ===\n",
      "Search results for 'suicide prevention techniques':\n",
      "  Result 1: by mining out depressions from their posts, as it has been trained\n",
      "on 2,590 Bangla data. This immensely useful system has been\n",
      "trained with the BERT model with 92% accuracy after analysing\n",
      "models like...\n",
      "  Result 2: Department of Computer Science and Engineering\n",
      "Northern University of Bangladesh\n",
      "Dhaka, Bangladesh\n",
      "sanjida.nub@gmail.com\n",
      "6th Shinthi Tasnim Himi\n",
      "Department of Computer Science and Engineering\n",
      "Banglade...\n",
      "  Result 3: 10.1080/09720502.2020.1721674\n",
      "[21] F. Haque, R. U. Nur, S. A. Jahan, Z. Mahmud, and F. M. Shah, A\n",
      "Transformer Based Approach To Detect Suicidal Ideation Using Pre-\n",
      "Trained Language Models. 2020. doi: ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from typing import List, Optional, Dict, Any\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manager for vector store operations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = EmbeddingFactory.get_embeddings()\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def load_vectorstore(self, path: Optional[str] = None) -> bool:\n",
    "        \"\"\"Load vector store from path\"\"\"\n",
    "        try:\n",
    "            path = path or settings.VECTORSTORE_PATH\n",
    "            if os.path.exists(path):\n",
    "                self.vectorstore = FAISS.load_local(path, self.embeddings)\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vectorstore: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search_documents(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            return [doc.page_content for doc in docs]\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def add_documents(self, texts: List[str], metadatas: Optional[List[dict]] = None):\n",
    "        \"\"\"Add documents to vector store\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            self.vectorstore = FAISS.from_texts(texts, self.embeddings, metadatas=metadatas)\n",
    "        else:\n",
    "            self.vectorstore.add_texts(texts, metadatas=metadatas)\n",
    "    \n",
    "    def save_vectorstore(self, path: Optional[str] = None):\n",
    "        \"\"\"Save vector store to path\"\"\"\n",
    "        if self.vectorstore:\n",
    "            path = path or settings.VECTORSTORE_PATH\n",
    "            self.vectorstore.save_local(path)\n",
    "\n",
    "\n",
    "def store_documents_in_vectorstore(\n",
    "    file_paths: List[str],\n",
    "    vectorstore_manager: Optional[VectorStoreManager] = None,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    save_path: Optional[str] = None,\n",
    "    include_metadata: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process documents and store them in vector store\n",
    "    \n",
    "    Args:\n",
    "        file_paths (List[str]): List of file paths to process\n",
    "        vectorstore_manager (VectorStoreManager, optional): Existing manager instance\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        save_path (str, optional): Path to save the vector store\n",
    "        include_metadata (bool): Whether to include file metadata\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Processing results with statistics\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    if vectorstore_manager is None:\n",
    "        vectorstore_manager = VectorStoreManager()\n",
    "    \n",
    "    chunker = DocumentChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    # Load existing vectorstore if available\n",
    "    vectorstore_manager.load_vectorstore(save_path)\n",
    "    \n",
    "    # Track processing statistics\n",
    "    results = {\n",
    "        \"total_files\": len(file_paths),\n",
    "        \"processed_files\": 0,\n",
    "        \"failed_files\": [],\n",
    "        \"total_chunks\": 0,\n",
    "        \"chunks_by_file\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                \n",
    "                # Process file into chunks\n",
    "                chunks = chunker.process_file(file_path)\n",
    "                \n",
    "                if chunks:\n",
    "                    # Prepare metadata if requested\n",
    "                    metadatas = None\n",
    "                    if include_metadata:\n",
    "                        file_name = Path(file_path).name\n",
    "                        file_extension = Path(file_path).suffix\n",
    "                        metadatas = [\n",
    "                            {\n",
    "                                \"source\": file_path,\n",
    "                                \"file_name\": file_name,\n",
    "                                \"file_extension\": file_extension,\n",
    "                                \"chunk_index\": i\n",
    "                            }\n",
    "                            for i in range(len(chunks))\n",
    "                        ]\n",
    "                    \n",
    "                    # Add documents to vector store\n",
    "                    vectorstore_manager.add_documents(chunks, metadatas)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    results[\"processed_files\"] += 1\n",
    "                    results[\"total_chunks\"] += len(chunks)\n",
    "                    results[\"chunks_by_file\"][file_path] = len(chunks)\n",
    "                    \n",
    "                    print(f\"Successfully processed {file_path}: {len(chunks)} chunks\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"No chunks extracted from {file_path}\")\n",
    "                    results[\"failed_files\"].append(file_path)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                results[\"failed_files\"].append(file_path)\n",
    "        \n",
    "        # Save the vector store\n",
    "        if results[\"total_chunks\"] > 0:\n",
    "            vectorstore_manager.save_vectorstore(save_path)\n",
    "            print(f\"Vector store saved with {results['total_chunks']} total chunks\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in store_documents_in_vectorstore: {e}\")\n",
    "        results[\"error\"] = str(e)\n",
    "        return results\n",
    "\n",
    "\n",
    "def store_single_document_in_vectorstore(\n",
    "    file_path: str,\n",
    "    vectorstore_manager: Optional[VectorStoreManager] = None,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    save_path: Optional[str] = None\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Process and store a single document in vector store\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to process\n",
    "        vectorstore_manager (VectorStoreManager, optional): Existing manager instance\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        save_path (str, optional): Path to save the vector store\n",
    "        \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    results = store_documents_in_vectorstore(\n",
    "        file_paths=[file_path],\n",
    "        vectorstore_manager=vectorstore_manager,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    return results[\"processed_files\"] > 0\n",
    "\n",
    "\n",
    "def batch_store_documents(\n",
    "    directory_path: str,\n",
    "    file_extensions: List[str] = [\".pdf\", \".docx\", \".txt\", \".md\"],\n",
    "    vectorstore_manager: Optional[VectorStoreManager] = None,\n",
    "    chunk_size: int = 1000,\n",
    "    chunk_overlap: int = 200,\n",
    "    save_path: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process and store all documents from a directory\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing documents\n",
    "        file_extensions (List[str]): List of file extensions to process\n",
    "        vectorstore_manager (VectorStoreManager, optional): Existing manager instance\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        save_path (str, optional): Path to save the vector store\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Any]: Processing results\n",
    "    \"\"\"\n",
    "    # Find all files with specified extensions\n",
    "    directory = Path(directory_path)\n",
    "    file_paths = []\n",
    "    \n",
    "    for extension in file_extensions:\n",
    "        file_paths.extend(directory.glob(f\"*{extension}\"))\n",
    "    \n",
    "    # Convert to string paths\n",
    "    file_paths = [str(path) for path in file_paths]\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"No files found in {directory_path} with extensions {file_extensions}\")\n",
    "        return {\"total_files\": 0, \"processed_files\": 0, \"failed_files\": [], \"total_chunks\": 0}\n",
    "    \n",
    "    print(f\"Found {len(file_paths)} files to process\")\n",
    "    \n",
    "    return store_documents_in_vectorstore(\n",
    "        file_paths=file_paths,\n",
    "        vectorstore_manager=vectorstore_manager,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    \"\"\"Example usage of the vector store functions\"\"\"\n",
    "    \n",
    "    # Initialize vector store manager\n",
    "    vs_manager = VectorStoreManager()\n",
    "    \n",
    "    # Example 1: Store a single document\n",
    "    print(\"=== Storing Single Document ===\")\n",
    "    file_path = \"/home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/SuicideGuard_An_NLP-Based_Chrome_Extension_for_Detecting_Suicidal_Thoughts_in_Bengali.pdf\"\n",
    "    success = store_single_document_in_vectorstore(\n",
    "        file_path=file_path,\n",
    "        vectorstore_manager=vs_manager,\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150\n",
    "    )\n",
    "    print(f\"Single document processing: {'Success' if success else 'Failed'}\")\n",
    "    \n",
    "    # # Example 2: Store multiple documents\n",
    "    # print(\"\\n=== Storing Multiple Documents ===\")\n",
    "    # file_paths = [\n",
    "    #     \"document1.pdf\",\n",
    "    #     \"document2.docx\",\n",
    "    #     \"document3.txt\"\n",
    "    # ]\n",
    "    \n",
    "    # results = store_documents_in_vectorstore(\n",
    "    #     file_paths=file_paths,\n",
    "    #     vectorstore_manager=vs_manager,\n",
    "    #     chunk_size=1000,\n",
    "    #     chunk_overlap=200\n",
    "    # )\n",
    "    \n",
    "    # print(f\"Processing Results:\")\n",
    "    # print(f\"  Total files: {results['total_files']}\")\n",
    "    # print(f\"  Processed files: {results['processed_files']}\")\n",
    "    # print(f\"  Failed files: {results['failed_files']}\")\n",
    "    # print(f\"  Total chunks: {results['total_chunks']}\")\n",
    "    \n",
    "    # # Example 3: Batch process directory\n",
    "    # print(\"\\n=== Batch Processing Directory ===\")\n",
    "    # directory_path = \"/home/ubuntu/OMANI-Therapist-Voice-ChatBot/KnowledgebaseFile/\"\n",
    "    \n",
    "    # batch_results = batch_store_documents(\n",
    "    #     directory_path=directory_path,\n",
    "    #     file_extensions=[\".pdf\", \".docx\", \".txt\", \".md\"],\n",
    "    #     vectorstore_manager=vs_manager\n",
    "    # )\n",
    "    \n",
    "    # print(f\"Batch Processing Results:\")\n",
    "    # print(f\"  Total files: {batch_results['total_files']}\")\n",
    "    # print(f\"  Processed files: {batch_results['processed_files']}\")\n",
    "    # print(f\"  Total chunks: {batch_results['total_chunks']}\")\n",
    "    \n",
    "    # Example 4: Search the vector store\n",
    "    print(\"\\n=== Searching Vector Store ===\")\n",
    "    query = \"suicide prevention techniques\"\n",
    "    search_results = vs_manager.search_documents(query, k=3)\n",
    "    \n",
    "    print(f\"Search results for '{query}':\")\n",
    "    for i, result in enumerate(search_results):\n",
    "        print(f\"  Result {i+1}: {result[:200]}...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b5929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    \"\"\"Request schema for query processing\"\"\"\n",
    "    query: str\n",
    "    session_id: Optional[str] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class QueryResponse(BaseModel):\n",
    "    \"\"\"Response schema for query processing\"\"\"\n",
    "    query: str\n",
    "    upgraded_query: str\n",
    "    route_taken: str\n",
    "    response: str\n",
    "    metadata: Dict[str, Any]\n",
    "    processing_time: float\n",
    "\n",
    "class ProcessingMetadata(BaseModel):\n",
    "    \"\"\"Metadata for processing steps\"\"\"\n",
    "    upgrade_success: bool = False\n",
    "    routing_success: bool = False\n",
    "    path_success: bool = False\n",
    "    errors: List[str] = []\n",
    "    processing_time: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56a10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"State schema for the AgenticRAG workflow\"\"\"\n",
    "    \n",
    "    user_query: str = Field(description=\"Original user query\")\n",
    "    upgraded_query: str = Field(default=\"\", description=\"Enhanced query\")\n",
    "    route_decision: str = Field(default=\"\", description=\"Routing decision\")\n",
    "    retrieved_docs: List[str] = Field(default_factory=list, description=\"Retrieved documents\")\n",
    "    search_results: List[str] = Field(default_factory=list, description=\"Web search results\")\n",
    "    final_response: str = Field(default=\"\", description=\"Final response\")\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Pydantic configuration\"\"\"\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4e83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class Prompts:\n",
    "    \"\"\"Centralized prompt templates\"\"\"\n",
    "    \n",
    "    QUERY_UPGRADER = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a query enhancement specialist. Your task is to improve user queries for better information retrieval.\n",
    "        \n",
    "        Enhancement guidelines:\n",
    "        1. Add relevant keywords and synonyms\n",
    "        2. Clarify ambiguous terms\n",
    "        3. Expand abbreviations and acronyms\n",
    "        4. Add context when missing\n",
    "        5. Maintain original intent\n",
    "        6. Keep enhanced query concise (under 200 characters)\n",
    "        \n",
    "        Return only the enhanced query, nothing else.\"\"\"),\n",
    "        (\"human\", \"Original query: {query}\")\n",
    "    ])\n",
    "    \n",
    "    QUERY_ROUTER = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a query router. Analyze the query and decide which path to take:\n",
    "\n",
    "        PATHS:\n",
    "        1. \"RAG\" - For queries about specific knowledge base content, documents, or domain expertise\n",
    "        2. \"WEB\" - For current events, real-time information, recent news, or trending topics\n",
    "        3. \"DIRECT\" - For general conversation, creative tasks, opinions, or reasoning without specific facts\n",
    "\n",
    "        DECISION CRITERIA:\n",
    "        - RAG: Domain-specific questions, technical documentation, specific facts from knowledge base\n",
    "        - WEB: Questions with temporal keywords (latest, current, recent, today), current events, real-time data\n",
    "        - DIRECT: General chat, creative writing, opinions, mathematical reasoning, casual conversation\n",
    "\n",
    "        Respond with only one word: RAG, WEB, or DIRECT\"\"\"),\n",
    "        (\"human\", \"Query: {query}\")\n",
    "    ])\n",
    "    \n",
    "    RAG_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the user's question based on the provided context from the knowledge base.\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        If the context doesn't contain relevant information, say so clearly.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    WEB_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the user's question based on the provided web search results.\n",
    "        \n",
    "        Search Results: {search_results}\n",
    "        \n",
    "        Provide a comprehensive answer based on the search results. If the results don't contain relevant information, say so clearly.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "    \n",
    "    DIRECT_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant. Answer the user's question directly using your knowledge and reasoning capabilities.\n",
    "        \n",
    "        Be conversational, accurate, and helpful. If you're unsure about something, acknowledge the uncertainty.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58e6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectLLMNode:\n",
    "    \"\"\"Node for direct LLM processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.prompt = Prompts.DIRECT_RESPONSE\n",
    "    \n",
    "    def process_direct_llm(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process direct LLM path\"\"\"\n",
    "        \n",
    "        try:\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            response = chain.invoke({\"query\": state.upgraded_query})\n",
    "            state.final_response = response.content\n",
    "            state.metadata[\"direct_llm_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.final_response = \"Sorry, I couldn't process your request at the moment.\"\n",
    "            state.metadata[\"direct_llm_success\"] = False\n",
    "            state.metadata[\"direct_llm_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def direct_llm_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for direct LLM processing\"\"\"\n",
    "    direct_processor = DirectLLMNode()\n",
    "    return direct_processor.process_direct_llm(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e0c087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRouter:\n",
    "    \"\"\"Node for routing queries to appropriate paths\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.prompt = Prompts.QUERY_ROUTER\n",
    "    \n",
    "    def route_query(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Route query to appropriate path\"\"\"\n",
    "        \n",
    "        chain = self.prompt | self.llm\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": state.upgraded_query})\n",
    "            route_decision = response.content.strip().upper()\n",
    "            \n",
    "            # Validate route decision\n",
    "            if route_decision not in [\"RAG\", \"WEB\", \"DIRECT\"]:\n",
    "                route_decision = settings.DEFAULT_ROUTE\n",
    "                \n",
    "            state.route_decision = route_decision\n",
    "            state.metadata[\"routing_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.route_decision = settings.DEFAULT_ROUTE\n",
    "            state.metadata[\"routing_success\"] = False\n",
    "            state.metadata[\"routing_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def query_router_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for query routing\"\"\"\n",
    "    router = QueryRouter()\n",
    "    return router.route_query(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c517d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryUpgrader:\n",
    "    \"\"\"Node for upgrading user queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.prompt = Prompts.QUERY_UPGRADER\n",
    "    \n",
    "    def upgrade_query(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Upgrade/enhance the user query\"\"\"\n",
    "        \n",
    "        chain = self.prompt | self.llm\n",
    "        \n",
    "        try:\n",
    "            response = chain.invoke({\"query\": state.user_query})\n",
    "            upgraded_query = response.content.strip()\n",
    "            \n",
    "            # Fallback to original if upgrade fails\n",
    "            if not upgraded_query or len(upgraded_query) > settings.MAX_QUERY_LENGTH:\n",
    "                upgraded_query = state.user_query\n",
    "                \n",
    "            state.upgraded_query = upgraded_query\n",
    "            state.metadata[\"upgrade_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.upgraded_query = state.user_query\n",
    "            state.metadata[\"upgrade_success\"] = False\n",
    "            state.metadata[\"upgrade_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def query_upgrader_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for query upgrading\"\"\"\n",
    "    upgrader = QueryUpgrader()\n",
    "    return upgrader.upgrade_query(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a93ee406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGNode:\n",
    "    \"\"\"Node for RAG processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.vectorstore_manager = VectorStoreManager()\n",
    "        self.prompt = Prompts.RAG_RESPONSE\n",
    "        \n",
    "        # Load vectorstore\n",
    "        self.vectorstore_manager.load_vectorstore()\n",
    "    \n",
    "    def process_rag(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process RAG path - retrieve from knowledge base\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            docs = self.vectorstore_manager.search_documents(state.upgraded_query, k=3)\n",
    "            state.retrieved_docs = docs\n",
    "            \n",
    "            # Generate response with retrieved context\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            context = \"\\n\".join(docs) if docs else \"No relevant documents found.\"\n",
    "            response = chain.invoke({\n",
    "                \"query\": state.upgraded_query,\n",
    "                \"context\": context\n",
    "            })\n",
    "            \n",
    "            state.final_response = response.content\n",
    "            state.metadata[\"rag_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.final_response = \"Sorry, I couldn't retrieve information from the knowledge base.\"\n",
    "            state.metadata[\"rag_success\"] = False\n",
    "            state.metadata[\"rag_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def rag_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for RAG processing\"\"\"\n",
    "    rag_processor = RAGNode()\n",
    "    return rag_processor.process_rag(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13311fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchNode:\n",
    "    \"\"\"Node for web search processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = LLMFactory.get_llm()\n",
    "        self.search_tool = SearchToolFactory.get_search_tool()\n",
    "        self.prompt = Prompts.WEB_RESPONSE\n",
    "    \n",
    "    def process_web_search(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Process web search path\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Perform web search\n",
    "            search_results = self.search_tool.run(state.upgraded_query)\n",
    "            state.search_results = [search_results]\n",
    "            \n",
    "            # Generate response with search results\n",
    "            chain = self.prompt | self.llm\n",
    "            \n",
    "            response = chain.invoke({\n",
    "                \"query\": state.upgraded_query,\n",
    "                \"search_results\": search_results\n",
    "            })\n",
    "            \n",
    "            state.final_response = response.content\n",
    "            state.metadata[\"web_search_success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            state.final_response = \"Sorry, I couldn't perform web search at the moment.\"\n",
    "            state.metadata[\"web_search_success\"] = False\n",
    "            state.metadata[\"web_search_error\"] = str(e)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Node function for LangGraph\n",
    "def web_search_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for web search processing\"\"\"\n",
    "    web_processor = WebSearchNode()\n",
    "    return web_processor.process_web_search(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bbcdd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(state: AgentState) -> Literal[\"rag_path\", \"web_search\", \"direct_llm\"]:\n",
    "    \"\"\"Route to appropriate path based on decision\"\"\"\n",
    "    route_map = {\n",
    "        \"RAG\": \"rag_path\",\n",
    "        \"WEB\": \"web_search\", \n",
    "        \"DIRECT\": \"direct_llm\"\n",
    "    }\n",
    "    return route_map.get(state.route_decision, \"direct_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe42640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class GraphBuilder:\n",
    "    \"\"\"Builder for the AgenticRAG graph\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_graph():\n",
    "        \"\"\"Create the LangGraph workflow\"\"\"\n",
    "        \n",
    "        # Initialize graph\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"query_upgrader\", query_upgrader_node)\n",
    "        workflow.add_node(\"query_router\", query_router_node)\n",
    "        workflow.add_node(\"rag_path\", rag_node)\n",
    "        workflow.add_node(\"web_search\", web_search_node)\n",
    "        workflow.add_node(\"direct_llm\", direct_llm_node)\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"query_upgrader\")\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"query_upgrader\", \"query_router\")\n",
    "        \n",
    "        # Add conditional edges based on routing decision\n",
    "        workflow.add_conditional_edges(\n",
    "            \"query_router\",\n",
    "            route_query,\n",
    "            {\n",
    "                \"rag_path\": \"rag_path\",\n",
    "                \"web_search\": \"web_search\",\n",
    "                \"direct_llm\": \"direct_llm\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # All paths end at END\n",
    "        workflow.add_edge(\"rag_path\", END)\n",
    "        workflow.add_edge(\"web_search\", END)\n",
    "        workflow.add_edge(\"direct_llm\", END)\n",
    "        \n",
    "        # Compile the graph\n",
    "        return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b304f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-12 19:44:25.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mAgenticRAG system initialized successfully\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:25.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing query: What is machine learning?\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:25.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mInitial state created: user_query='What is machine learning?' upgraded_query='' route_decision='' retrieved_docs=[] search_results=[] final_response='' metadata={}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: What is machine learning?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-12 19:44:26.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mFinal state after processing: {'user_query': 'What is machine learning?', 'upgraded_query': 'What is artificial intelligence machine learning algorithm?', 'route_decision': 'RAG', 'retrieved_docs': [], 'search_results': [], 'final_response': \"I apologize, but since there are no relevant documents found in my knowledge base, I don't have any information on what artificial intelligence machine learning algorithm is. If you're looking for information on this topic, I suggest searching online or consulting a reliable source.\", 'metadata': {'upgrade_success': True, 'routing_success': True, 'rag_success': True}}\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:26.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mQuery processed successfully in 1.08s\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:26.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing query: Latest news about AI\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:26.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mInitial state created: user_query='Latest news about AI' upgraded_query='' route_decision='' retrieved_docs=[] search_results=[] final_response='' metadata={}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What is machine learning?\n",
      "Upgraded Query: What is artificial intelligence machine learning algorithm?\n",
      "Route Taken: RAG\n",
      "Response: I apologize, but since there are no relevant documents found in my knowledge base, I don't have any information on what artificial intelligence machine learning algorithm is. If you're looking for information on this topic, I suggest searching online or consulting a reliable source.\n",
      "Processing Time: 1.08s\n",
      "Metadata: {'upgrade_success': True, 'routing_success': True, 'rag_success': True}\n",
      "\n",
      "==================================================\n",
      "Query: Latest news about AI\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-12 19:44:28.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mFinal state after processing: {'user_query': 'Latest news about AI', 'upgraded_query': 'Enhanced query: Latest artificial intelligence news updates', 'route_decision': 'WEB', 'retrieved_docs': [], 'search_results': [{'query': 'Enhanced query: Latest artificial intelligence news updates', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.artificialintelligence-news.com/', 'title': 'AI News | Latest AI News, Analysis & Events', 'content': 'AI News reports on the latest artificial intelligence news and insights. Explore industry trends from the frontline of AI.', 'score': 0.5806618, 'raw_content': None}, {'url': 'https://www.sciencedaily.com/news/computers_math/artificial_intelligence/', 'title': 'Artificial Intelligence News - ScienceDaily', 'content': 'Artificial Intelligence News. Everything on AI including futuristic robots with artificial intelligence, computer models of human intelligence and more.', 'score': 0.54187006, 'raw_content': None}, {'url': 'https://www.crescendo.ai/news/latest-ai-news-and-updates', 'title': 'Latest AI Breakthroughs and News: May, June, July 2025', 'content': \"Wondering what's happening in the AI world? Here are the latest AI breakthroughs and news that are shaping the world around us!\", 'score': 0.47446132, 'raw_content': None}, {'url': 'https://blog.google/technology/ai/google-ai-updates-june-2025/', 'title': 'The latest AI news we announced in June - Google Blog', 'content': 'Google made some cool AI updates in June. They made their AI models faster and cheaper for people to use. You can now use your voice to search', 'score': 0.38609254, 'raw_content': None}, {'url': 'https://blog.google/technology/ai/google-ai-updates-may-2025/', 'title': 'The latest AI news we announced in May - Google Blog', 'content': \"Here's a recap of some of our biggest AI updates from May, including the releases at I/O and Google Marketing Live, news about AI Mode, new capabilities in the\", 'score': 0.3408586, 'raw_content': None}], 'response_time': 0.99}], 'final_response': \"Based on the search results, here's a comprehensive answer to your query:\\n\\nThe latest artificial intelligence news updates can be found on various websites and blogs that report on the latest developments and breakthroughs in the field of AI. Some of the top sources for AI news include AI News, ScienceDaily, Crescendo AI, and Google's official blog.\\n\\nAccording to AI News, the latest AI news and insights can be explored on their website, which reports on industry trends from the frontline of AI. ScienceDaily also provides a comprehensive section on artificial intelligence news, covering topics such as futuristic robots with AI, computer models of human intelligence, and more.\\n\\nCrescendo AI recently published a blog post highlighting the latest AI breakthroughs and news from May, June, and July 2025. The post covers various developments in the field, including advancements in AI models, new applications of AI, and more.\\n\\nGoogle's official blog has also published several updates on their AI developments, including news on their AI models becoming faster and cheaper for people to use, as well as the introduction of voice search capabilities.\\n\\nSome of the key takeaways from these sources include:\\n\\n* Advancements in AI models and their applications\\n* New developments in AI-powered robots and machines\\n* Increased focus on making AI more accessible and affordable for people\\n* Growing use of AI in various industries, including healthcare, finance, and more\\n\\nOverall, the latest AI news updates suggest that the field of artificial intelligence is rapidly evolving, with new breakthroughs and developments emerging regularly.\", 'metadata': {'upgrade_success': True, 'routing_success': True, 'web_search_success': True}}\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:28.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mQuery processed successfully in 2.42s\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:28.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing query: Write a poem about spring\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:28.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mInitial state created: user_query='Write a poem about spring' upgraded_query='' route_decision='' retrieved_docs=[] search_results=[] final_response='' metadata={}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Latest news about AI\n",
      "Upgraded Query: Enhanced query: Latest artificial intelligence news updates\n",
      "Route Taken: WEB\n",
      "Response: Based on the search results, here's a comprehensive answer to your query:\n",
      "\n",
      "The latest artificial intelligence news updates can be found on various websites and blogs that report on the latest developments and breakthroughs in the field of AI. Some of the top sources for AI news include AI News, ScienceDaily, Crescendo AI, and Google's official blog.\n",
      "\n",
      "According to AI News, the latest AI news and insights can be explored on their website, which reports on industry trends from the frontline of AI. ScienceDaily also provides a comprehensive section on artificial intelligence news, covering topics such as futuristic robots with AI, computer models of human intelligence, and more.\n",
      "\n",
      "Crescendo AI recently published a blog post highlighting the latest AI breakthroughs and news from May, June, and July 2025. The post covers various developments in the field, including advancements in AI models, new applications of AI, and more.\n",
      "\n",
      "Google's official blog has also published several updates on their AI developments, including news on their AI models becoming faster and cheaper for people to use, as well as the introduction of voice search capabilities.\n",
      "\n",
      "Some of the key takeaways from these sources include:\n",
      "\n",
      "* Advancements in AI models and their applications\n",
      "* New developments in AI-powered robots and machines\n",
      "* Increased focus on making AI more accessible and affordable for people\n",
      "* Growing use of AI in various industries, including healthcare, finance, and more\n",
      "\n",
      "Overall, the latest AI news updates suggest that the field of artificial intelligence is rapidly evolving, with new breakthroughs and developments emerging regularly.\n",
      "Processing Time: 2.42s\n",
      "Metadata: {'upgrade_success': True, 'routing_success': True, 'web_search_success': True}\n",
      "\n",
      "==================================================\n",
      "Query: Write a poem about spring\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-12 19:44:30.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mFinal state after processing: {'user_query': 'Write a poem about spring', 'upgraded_query': 'Enhanced query: Write a poem about the arrival of spring season', 'route_decision': 'DIRECT', 'retrieved_docs': [], 'search_results': [], 'final_response': \"As winter's chill begins to fade,\\nThe earth awakens from its shade,\\nThe sun shines bright with warmer rays,\\nAnd spring's sweet symphony sways.\\n\\nThe trees, once bare and branch-like gray,\\nNow don a cloak of green array,\\nTheir leaves unfurl, a vibrant hue,\\nAs nature's palette is anew.\\n\\nThe air is filled with scents so sweet,\\nOf blooming flowers, fresh and neat,\\nThe hum of bees, a gentle thrum,\\nAs they collect nectar, one by one.\\n\\nThe world, once frozen, now comes alive,\\nWith chirping birds, a joyful jive,\\nTheir songs, a chorus, loud and clear,\\nAs spring's arrival banishes all fear.\\n\\nThe snow, that once lay deep and wide,\\nNow melts away, a gentle tide,\\nRevealing hidden streams and brooks,\\nThat babble, gurgle, and gentle crooks.\\n\\nThe earth, once barren, now is green,\\nA canvas, vibrant, fresh, and serene,\\nAs spring's arrival brings new birth,\\nAnd all around, new life on earth.\\n\\nSo let us welcome spring's sweet charm,\\nWith open hearts, and arms disarm,\\nFor in its arrival, we find peace,\\nAnd a world, reborn, in all its release.\", 'metadata': {'upgrade_success': True, 'routing_success': True, 'direct_llm_success': True}}\u001b[0m\n",
      "\u001b[32m2025-07-12 19:44:30.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mQuery processed successfully in 1.13s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Write a poem about spring\n",
      "Upgraded Query: Enhanced query: Write a poem about the arrival of spring season\n",
      "Route Taken: DIRECT\n",
      "Response: As winter's chill begins to fade,\n",
      "The earth awakens from its shade,\n",
      "The sun shines bright with warmer rays,\n",
      "And spring's sweet symphony sways.\n",
      "\n",
      "The trees, once bare and branch-like gray,\n",
      "Now don a cloak of green array,\n",
      "Their leaves unfurl, a vibrant hue,\n",
      "As nature's palette is anew.\n",
      "\n",
      "The air is filled with scents so sweet,\n",
      "Of blooming flowers, fresh and neat,\n",
      "The hum of bees, a gentle thrum,\n",
      "As they collect nectar, one by one.\n",
      "\n",
      "The world, once frozen, now comes alive,\n",
      "With chirping birds, a joyful jive,\n",
      "Their songs, a chorus, loud and clear,\n",
      "As spring's arrival banishes all fear.\n",
      "\n",
      "The snow, that once lay deep and wide,\n",
      "Now melts away, a gentle tide,\n",
      "Revealing hidden streams and brooks,\n",
      "That babble, gurgle, and gentle crooks.\n",
      "\n",
      "The earth, once barren, now is green,\n",
      "A canvas, vibrant, fresh, and serene,\n",
      "As spring's arrival brings new birth,\n",
      "And all around, new life on earth.\n",
      "\n",
      "So let us welcome spring's sweet charm,\n",
      "With open hearts, and arms disarm,\n",
      "For in its arrival, we find peace,\n",
      "And a world, reborn, in all its release.\n",
      "Processing Time: 1.13s\n",
      "Metadata: {'upgrade_success': True, 'routing_success': True, 'direct_llm_success': True}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from loguru import logger\n",
    "\n",
    "class AgenticRAGSystem:\n",
    "    \"\"\"Main AgenticRAG system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Validate settings\n",
    "        settings.validate()\n",
    "        \n",
    "        # Create graph\n",
    "        self.app = GraphBuilder.create_graph()\n",
    "        \n",
    "        logger.info(\"AgenticRAG system initialized successfully\")\n",
    "    \n",
    "    def process_query(self, query: str) -> QueryResponse:\n",
    "        \"\"\"Process a single query\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "\n",
    "        try:\n",
    "            # Initialize state\n",
    "            initial_state = AgentState(user_query=query)\n",
    "            logger.info(f\"Initial state created: {initial_state}\")\n",
    "            \n",
    "            # Run the graph\n",
    "            final_state = self.app.invoke(initial_state)\n",
    "            logger.info(f\"Final state after processing: {final_state}\")\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Create response\n",
    "            response = QueryResponse(\n",
    "                query=final_state['user_query'],\n",
    "                upgraded_query=final_state['upgraded_query'],\n",
    "                route_taken=final_state['route_decision'],\n",
    "                response=final_state['final_response'],\n",
    "                metadata=final_state['metadata'],\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Query processed successfully in {processing_time:.2f}s\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def process_batch(self, queries: List[str]) -> List[QueryResponse]:\n",
    "        \"\"\"Process multiple queries\"\"\"\n",
    "        \n",
    "        responses = []\n",
    "        for query in queries:\n",
    "            try:\n",
    "                response = self.process_query(query)\n",
    "                responses.append(response)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing query '{query}': {e}\")\n",
    "        \n",
    "        return responses\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    \n",
    "    # Initialize system\n",
    "    system = AgenticRAGSystem()\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Latest news about AI\",\n",
    "        \"Write a poem about spring\"\n",
    "    ]\n",
    "    \n",
    "    # Process queries\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            response = system.process_query(query)\n",
    "            \n",
    "            print(f\"Original Query: {response.query}\")\n",
    "            print(f\"Upgraded Query: {response.upgraded_query}\")\n",
    "            print(f\"Route Taken: {response.route_taken}\")\n",
    "            print(f\"Response: {response.response}\")\n",
    "            print(f\"Processing Time: {response.processing_time:.2f}s\")\n",
    "            print(f\"Metadata: {response.metadata}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ee3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a query router. Analyze the query and decide which path to take:\n",
      "\n",
      "PATHS:\n",
      "1. \"RAG\" - For queries about specific knowledge base content, documents, or domain expertise\n",
      "2. \"WEB\" - For current events, real-time information, recent news, or trending topics\n",
      "3. \"DIRECT\" - For general conversation, creative tasks, opinions, or reasoning without specific facts\n",
      "\n",
      "DECISION CRITERIA:\n",
      "- RAG: Domain-specific questions, technical documentation, specific facts from knowledge base\n",
      "- WEB: Questions with temporal keywords (latest, current, recent, today), current events, real-time data\n",
      "- DIRECT: General chat, creative writing, opinions, mathematical reasoning, casual conversation\n",
      "\n",
      "Knowledge Base contains:\n",
      "- resume of an AI developer\n",
      "\n",
      "Respond with only one word: RAG, WEB, or DIRECT\n",
      "Human: Query: {query}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "\n",
    "class Prompts:\n",
    "    \"\"\"Centralized prompt templates\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_kb_description():\n",
    "        \"\"\"Dynamically load knowledge base descriptions\"\"\"\n",
    "        try:\n",
    "            with open(\"knowledge_base_metadata.json\", 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            description = \"\"\n",
    "            for item in data:\n",
    "                description += f\"- {item.get('description', '').strip()}\\n\"\n",
    "            return description.strip()\n",
    "        except FileNotFoundError:\n",
    "            return \"No knowledge base found.\"\n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"Error decoding knowledge base: {e}\"\n",
    "\n",
    "    @classmethod\n",
    "    def query_router(cls):\n",
    "        \"\"\"Return QUERY_ROUTER with dynamic KB info\"\"\"\n",
    "        kb_description = cls.load_kb_description()\n",
    "        return ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are a query router. Analyze the query and decide which path to take:\n",
    "\n",
    "PATHS:\n",
    "1. \"RAG\" - For queries about specific knowledge base content, documents, or domain expertise\n",
    "2. \"WEB\" - For current events, real-time information, recent news, or trending topics\n",
    "3. \"DIRECT\" - For general conversation, creative tasks, opinions, or reasoning without specific facts\n",
    "\n",
    "DECISION CRITERIA:\n",
    "- RAG: Domain-specific questions, technical documentation, specific facts from knowledge base\n",
    "- WEB: Questions with temporal keywords (latest, current, recent, today), current events, real-time data\n",
    "- DIRECT: General chat, creative writing, opinions, mathematical reasoning, casual conversation\n",
    "\n",
    "Knowledge Base contains:\n",
    "{kb_description}\n",
    "\n",
    "Respond with only one word: RAG, WEB, or DIRECT\"\"\"),\n",
    "            (\"human\", \"Query: {{query}}\")\n",
    "        ])\n",
    "\n",
    "    # Other prompts stay unchanged\n",
    "    QUERY_UPGRADER = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a query enhancement specialist...\"\"\"),  # shortened for brevity\n",
    "        (\"human\", \"Original query: {query}\")\n",
    "    ])\n",
    "\n",
    "    RAG_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the user's question based on the provided context from the knowledge base.\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        If the context doesn't contain relevant information, say so clearly.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "    WEB_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant. Answer the user's question based on the provided web search results.\n",
    "        \n",
    "        Search Results: {search_results}\n",
    "        \n",
    "        Provide a comprehensive answer...\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "    DIRECT_RESPONSE = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant. Answer the user's question directly using your knowledge and reasoning capabilities.\"\"\"),\n",
    "        (\"human\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = Prompts.query_router()\n",
    "    print(prompt.format(query=\"What is the architecture of the Omani AI system?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a357d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
